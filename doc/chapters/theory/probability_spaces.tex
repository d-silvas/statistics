% TODO: sigma-algebras...

\subsection{Definition and basic properties}

\begin{definition}
	\textbf{Random Experiment:} an experiment whose outcomes are determined only by chance factors.
\end{definition}

\begin{definition}
	The description of a random experiment starts by identifying the set of all possible  outcomes of the experiment, which we call \textbf{sample space}, and we designate by $\O$.
	\begin{itemize}
		\item When $\O$ is finite or countable (\textit{numerable}), we say that the probability model is \textbf{discrete}.
		\item When $\O$ is uncountable, we say that the probability model is \textbf{continuous}.
	\end{itemize}
\end{definition}

\begin{definition}
	Let $\O$ be a set and $\F$ a $\sigma$-algebra on $\O$. We say that $(\O, \F)$ is a \textbf{measurable space}. Any subset belonging to $\F$ is called an \textbf{event}.
	\begin{itemize}
		\item When $\O$ is finite or countable, we will usually consider $\F = \mathcal{P}(\O)$.
		\item When $\O$ is uncountable, with $\O \subseteq \R^k$, we will usually consider $\F = \mathbb{B}_{\O}^k$.
	\end{itemize}
\end{definition}

\begin{definition}
	Given a measurable space $(\O, \F)$, a \textbf{probability}, or \textbf{probability measure}, is a function $\P: \F \rightarrow \R$ such that:
	\begin{enumerate}[i)]
		\item $\P(A) \geq 0$ for every $A \in \F$.
		\item For every countable collection of events $\{A_n\} \subset \F$, which are pairwise disjoint, we have that:
		\[
			\P \left(\cup_n A_n\right) = \sum_n \P(A_n)
		\]
		\item $\P(\O) = 1$
	\end{enumerate}
	If $\P$ is a probability in $(\O, \F)$, we call the triad $(\O, \F, \P)$ a \textbf{probability space}. For every event $A \in \F$, we call $\P(A)$ the \textbf{probability of $A$}.
\end{definition}

\begin{prop}
	Let $(\O, \F, \P)$ be a probability space, and $A, B \in \F$. Then:
	\[
		\P(A - B) = \P(A) - \P(A \cap B)
	\]
	If $B \subseteq A$, then
	\[
		\P(A - B) = \P(A) - \P(B)
	\]
	and, thus, $\P(B) \leq \P(A)$. In particular, it is $\P(A) \leq 1$ for every $A \in \F$, and:
	\[
		\P(A^c) = 1 - \P(A)
	\]
	And from this last equality we deduce that $\P(\emptyset) = 0$.
\end{prop}

\begin{prop}
	Let $(\O, \F, \P)$ be a probability space, and $A_1, \ldots, A_n \in \F$. Then,
	\[
		\P(A_1 \cup A_2) = \P(A_1) + \P(A_2) - \P(A_1 \cap A_2)
	\]
	and, more generally, the \textbf{inclusion-exclusion formula} holds:
	\[
		\begin{gathered}
		\P(\cup_{i = 1}^n A_i) = \sum_{i = 1}^n\P(A_i) - \sum_{1 \leq i \leq j \leq n}\P(A_i \cap A_j) + \sum_{1 \leq i \leq j \leq k \leq n} \P(A_i \cap A_j \cap A_k) \\ + \cdots + (-1)^{n - 1} \P(A_1 \cap A_2 \cap \cdots A_n)
		\end{gathered}
	\]
\end{prop}

\subsection{Conditional probability}

In a probability space $(\O, \F, \P)$, the fact of knowing that the outcome of the random experiment verifies a particular event $B$, affects the probability of all events.
% TODO: find better explanation ...

\begin{definition}
	Let $B \in \F$ be an event such that $\P(B) > 0$. For each $A \in \F$, we define the \textbf{conditional probability of $A$ given $B$} as:
	\[
		\P(A \mid B) = \frac{\P(A \cap B)}{\P(B)}
	\]
\end{definition}

The knowledge that the event $B$ has been verified transforms the original probability space into a new one, $(\O, \F, \P(\cdot \mid B))$. Because we also know that the event $B^c$ has not happened, we can consider a more relevant space, $(B, \F_B, \P_B)$, in which:
\[
	\F_B = \{ A \cap B \mid A \in \F \}
\]
is the new $\sigma$-algebra of events. We assign each of the new events, $C \in \F_B$, the probability:
\[
	\P_B(C) = \frac{\P(C)}{\P(B)} = \frac{\P(A \cap B)}{\P(B)}, \text{ if } C = A \cap B \text{ for some } A \in \F
\]

\begin{prop}
	Let $(\O, \F, \P)$ be a probability space. If $B \in \F$ is an event such that $\P(B) > 0$, then both $(\O, \F, \P(\cdot \mid B))$ and $(B, \F_B, \P_B)$ are probability spaces.
\end{prop}

\begin{prop}[Product rule or Chain rule]
	Let $\PS$ be a probability space, and $A_1, \ldots, A_n \in \F$ events such that $\P(\cap_{i = 1}^{n - 1} A_i) > 0$. Then,
	\[
		\P(\cap_{i = 1}^{n} A_i) = \P(A_1) \P(A_2 \mid A_1) \P(A_3 \mid A_2 \cap A_1) \cdots \P(A_n \mid \cap_{i = 1}^{n - 1} A_i)
	\]
\end{prop}
\begin{proof}
	By iteratively applying the definition of conditional probability, $\P(A \cap B) = \P(B) \P(A \mid B)$, we have that:
	\begin{align*}
		\P(\cap_{i = 1}^{n} A_i) &= \P(\cap_{i = 1}^{n - 1} A_i)\P(A_n \mid \cap_{i = 1}^{n - 1}) = \\
		 &= \P(\cap_{i = 1} ^{n - 2} A_i) \P(A_{n - 1} \mid \cap_{i = 1} ^{n - 2} A_i) \P(A_n \mid \cap_{i = 1}^{n - 1}) = \\
		 &= \cdots
	\end{align*}
\end{proof}

\begin{prop}[Law or formula of total probability]
	If ${B_n} \subset \F$ is a finite or countably infinite partition of the sample space (in other words, a set of pairwise disjoint events whose union is $\O$) and $\P(B_n) > 0$ for each $n$, then for any event $A \in \F$:
	\[
		\P(A) = \sum_n \P(B_n) \P(A \cap B_n)
	\]
\end{prop}
\begin{proof}
	\begin{align*}
		\P(A) &= \P(A \cap \O) = \P(A \cap (\cup_n B_n)) = \P(\cup_n (A \cap B_n)) = \sum_n \P(A \cap B_n) = \\ &= \sum_n \P(B_n) \P(A \cap B_n)
	\end{align*}
\end{proof}

\begin{prop}[Bayes' formula]
	If $A$ and $B$ are events in the probability space $\PS$ such that $\P(B) > 0$, then:
	\[
		\P(A \mid B) = \frac{\P(A) \, \P(B \mid A)}{\P(B)}
	\]
\end{prop}

\begin{proof}
	\[
		\P(A \mid B) = \frac{\P(A \cap B)}{\P(B)} = \frac{\P(A) \, \P(B \mid A)}{\P(B)}
	\]
\end{proof}

\subsection{Independence}

\begin{definition}
	Let $\PS$ be a probability space. $\{ A_i \}_{i \in I }$ are \textbf{independent events} if:
	\[
		\P (\cap_{i \in F} A_i) = \prod_{i \in F} \P(A_i)
	\]
	for every finite subset $F \subset I$.
\end{definition}

\begin{definition}
	Let $\PS$ be a probability space and $\{ \mathcal{C}_i \}_{i \in I}$ families of events, where $\mathcal{C}_i \subset \F$ for each $i \in I$. $\{ \mathcal{C}_i \}_{i \in I}$ are said to be \textbf{independent} if the events $\{ A_i \}_{i \in I }$ are independent for every election of $A_i \in \mathcal{C}_i$ for every $i \in I$.
\end{definition}

\begin{example}
	Exercise 3.7 ...
\end{example}